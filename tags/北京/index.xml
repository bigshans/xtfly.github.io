<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>北京 on 蘭陵N散記</title>
    <link>http://lanlingzi.cn/tags/%E5%8C%97%E4%BA%AC/index.xml</link>
    <description>Recent content in 北京 on 蘭陵N散記</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <atom:link href="http://lanlingzi.cn/tags/%E5%8C%97%E4%BA%AC/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>参加ArchSummit北京站感受</title>
      <link>http://lanlingzi.cn/post/technical/2015/1227_bj_archsummit/</link>
      <pubDate>Sun, 27 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>http://lanlingzi.cn/post/technical/2015/1227_bj_archsummit/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://q.infoqstatic.com/ASSZ2015/LOGO/AS-LOGO358x146.png&#34; alt=&#34;ArchSummit&#34; /&gt;&lt;/p&gt;

&lt;p&gt;参加ArchSummit北京站已有一周时间，一直没有时间来梳理一下。整体来说，这次的北京之行，不是很满意，可能是这类会议听多的原因，感觉ArchSummit的质量是越来越差了，没有什么新鲜感，觉得不值那6K的价格。&lt;/p&gt;

&lt;h2 id=&#34;组织不足&#34;&gt;组织不足&lt;/h2&gt;

&lt;p&gt;12月份的北京已是非常的干冷，可能由于我在南方呆久了，一到北京是极其地不适应，在北京三天多的时间，嘴唇开裂，到现在还没有完全好干净。离开北京的那一天，正好又感受了一下北京正宗的霾，帝都的人们活得真不容易啊。

为什么说ArchSummit组织不足呢？InfoQ也算是组织过多次大型会议的公司，但这一次比我之前参加InfoQ组织的任何会议都差，更无法与阿里组织的云栖会议相比。一个是以组织会议赚钱，一个是以个会议来打造生态。这次的ArchSummit是在北京国际个会议中心举行，每个分会场我都差不多的参加过，明显感觉组织不足:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个分会场演讲时，大门紧闭，空间质量非常的差，又没有充足的通风设备，感觉非常的窒息。&lt;/li&gt;
&lt;li&gt;工作人员能力不行，第一天下午，有几个分会议室由于投影没有准备好，拖时半个多小时，也不见中途主持人来了说一声，最后连声道歉都没有。&lt;/li&gt;
&lt;li&gt;几个分会场的投影效果差，灰蒙蒙的看不清楚。&lt;/li&gt;
&lt;li&gt;连个矿泉水瓶上都是广告，并且不是每个位置都摆放好水，而是需要自己去指定位置去拿。有的分会场甚于连矿泉水都没有见到，准备的份数太少，6K的价格连个水都喝不到。&lt;/li&gt;
&lt;li&gt;就餐地方太小（又是自助餐），效率低下，大量的人员挤在走廊上，我是差不多等了30多分钟才能进餐厅吃饭。大量的人员挤在一起存在安全风险。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两天的ArchSummit大会日程比较紧凑，再加上大多数时候有六个专题在并行，因此每个人能够真正去听的课程不会太多。我们也是只能选择地去听，但是每个演讲介绍不足，有些演讲名字高大上，听了之后，感觉有点上当，部分讲师存在水分，这里就不直说了。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;不过，参加ArchSummit大会，还是听到一些业内公司的技术分享，尤其是互联网企业，在应用新技术方面还是比较超前的。相对我们电信行业来说，我们遇到的问题有些是相似的，甚至部分问题的解决办法也与我们曾经想过的一些方案类似，只是他们早已经落地并且做到极致了。有很多东西对我们值得参考，可以说从开源使用、技术形态，运作方式，远远走在我们的前面了。&lt;/p&gt;

&lt;h2 id=&#34;paas平台&#34;&gt;PaaS平台&lt;/h2&gt;

&lt;p&gt;目前稍具规模的互联网公司，都会自建数据中心。而互联网的业务又有如下特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;业务需要快速上线，唯快不破&lt;/li&gt;
&lt;li&gt;业务形态众多，迭代周期快&lt;/li&gt;
&lt;li&gt;数据处理量大，海量请求和高并发的挑战&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;支撑业务的发布，上线，运维，都需要对业务应用的全生命周期管理，各个公司都有一套平台，他们或多或少都能称得上内部的PaaS平台。而PaaS平台核心：&lt;/p&gt;

&lt;h3 id=&#34;分布式框架&#34;&gt;分布式框架&lt;/h3&gt;

&lt;p&gt;首先是《蚂蚁金服金融级PaaS平台构建之道》分享，阿里在国内技术一直算是走到前列。这次带来的演讲，蚂蚁金服的分布式服务注册中心（DSR），与阿里其它系的Dubbo，HSF都差不多。他们的目标都要解决应用服务化后，服务注册发现问题，可以说是未来PaaS平台中，服务注册发现将成来PaaS的核心中的核心。&lt;/p&gt;

&lt;p&gt;后一场听了《主流容器SDN技术与微服务架构实践》，来自七牛的分享。虽然演讲的内容是容器的SDN技术（算不上大范围的SDN），也同时点到微服务架构。虽然他们所讲的容器方案都说是自研的，但整体上感觉与K8S的设计是相似，甚至像Pod之类的概念来也是借鉴来的。在容器环境下的同时也要解决分布式的服务发现问题，他们采用是DNS机制。服务路由上支持L4与L7的负载均衡，对业务无侵入。基于安全组的服务Discovery，虽然没听太明白，感觉跟K8S的Proxy机制是差不多的。&lt;/p&gt;

&lt;h3 id=&#34;中间件服务&#34;&gt;中间件服务&lt;/h3&gt;

&lt;p&gt;在《蚂蚁金服金融级PaaS平台构建之道》中初步介绍了分布式消息(DMS)、分布式数据源（DDS），分布式事务（DTS）的一些使用场景与技术特点。在云环境下，中间件服务必不可少，让业务应用只关注自己的业务逻辑。中间件服务要面对的是一个复杂、不断变化的计算环境。抽象出业务的公共能力服务化。使用中间件服务，可以简化业务应用在一些通用技术的成本，如数据一致性，安全控制，高性能，可靠性等。而中间件技术正在呈现出业务化、服务化、一体化的趋势发展。高可用性，自管理性，业务适应性是当前中间件服务面临的挑战。&lt;/p&gt;

&lt;h3 id=&#34;弹性扩展&#34;&gt;弹性扩展&lt;/h3&gt;

&lt;p&gt;在云计算中，引入虚拟化技术，采用弹性伸缩是老生常谈了，一键式按需弹性，基于性能采集的自动弹性。听了《微众银行基于自主可控技术的分布式架构实践》，给我对弹性带了新的思考。互联网+的应用是：海量用户，海量交易，海量数据。这要求对系统在架构设计上充分考虑容量的扩展性，性能的扩展性。&lt;/p&gt;

&lt;p&gt;微众的架构特点是分布式松耦合架构+一主两从节点强制同步的架构。在分布式松耦合架构是按客户群来水平分割，一个节点上涵盖多个客户业务。分布式多节点是分散风险，如果有节点受损，也是部分客户有影响。而每个节点上又采用一主两从节点强制同步，来提高整个系统的冗余。整个系统以客户为单元可控分布，将客户量、交易频繁度与系统负载之间的关系解耦。随着客户量增加或客户交易频繁度的增加,系统负载也会随着增加：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;横向扩展(Scale Out)解决用户量增加&lt;/li&gt;
&lt;li&gt;纵向扩展(Scale Up)解决交易频繁度增加&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;并且严格要求，横向扩展只能解决用户量问题，不能通过纵向扩展来解决用户量问题，反之亦然。&lt;/p&gt;

&lt;h3 id=&#34;容灾备份&#34;&gt;容灾备份&lt;/h3&gt;

&lt;p&gt;云计算环境下，容灾备份也是需要重点考虑的，容灾设计强调的是系统对外界环境影响具备快速响应能力，尤其是当发生灾难性事件并对IDC节点产生影响时，能够具备节点级别的快速恢复能力，保障系统的持续可用。像微众介绍IDC2.0中提到的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据库三中心集群化部署&lt;/li&gt;
&lt;li&gt;三数据副本强同步&lt;/li&gt;
&lt;li&gt;应用多中心多活部署&lt;/li&gt;
&lt;li&gt;应用多中心多实例多活部署&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;蚂蚁金服金服提到的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;两地三中心&lt;/li&gt;
&lt;li&gt;异地多活&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;支付宝有一个专题《支付宝的高可用与容灾架构演进》，我觉得有意思的是其中的单元化与容灾。单元化应该是微服务化中一种具体运用吧。什么是支付宝的单元化：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;核心业务,核心剥离：数据按照UserID拆分,多机房部署,调用封闭,部分数据,不共享&lt;/li&gt;
&lt;li&gt;非核心业务,长尾独立：不能按照UID拆分，核心不依赖长尾&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;单元化的实现思路：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;水平拆：交易、支付、账务等,每个单元只有部分数据&lt;/li&gt;
&lt;li&gt;上层单元化改造：从DB层往上延伸水平拆分概念,包括应用层到入口层&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在容灾同步上，是基于单元化的多中心同步，这已打破我们对原有容灾备份的认识，基于单元化的容灾同步，可以细粒度的控制，解决数据一致性和时效性问题：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;基于DB同步的数据复制：延时非敏感业务的异地复制方案;部分业务数据,可忍受3s时效性延迟(比如大部分的配置 数据)&lt;/li&gt;
&lt;li&gt;基于消息系统的数据复制：对于延时非常敏感的业务,更低延时的实现方案;上层基于应用进行复制,减少延时。底层 DB主备同步同时进行&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;高效运维&#34;&gt;高效运维&lt;/h3&gt;

&lt;p&gt;开发团队快节奏的版本迭代，以及服务的快速上线的要求，驱动着PaaS平台要提供出更为高效的运维服务。高效运维的思路是建立以 &lt;strong&gt;应用服务&lt;/strong&gt; 为核心的管理标准体系。把运维能力服务化(API)，使运维的能力无处不在。高效运维，综合几个公司的介绍主要需要如下几个系统设计：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;发布系统：负责应用服务的上线，应用服务的资源管理，扩容，权限管理，支持Beta发布，灰度升级。&lt;/li&gt;
&lt;li&gt;监控系统：通用+自定义监控配置,运维+开发可以时刻关注自己的服务状态和质量。&lt;/li&gt;
&lt;li&gt;全链路系统：复杂的分布式系统，一次点击，几十次的RPC调，需要全链路跟踪，出了问题,如何快 速定位到故障点。&lt;/li&gt;
&lt;li&gt;限流与降级：限流,Web层,防止被流量打垮；降级,App层(服务化),保障核心应用&lt;/li&gt;
&lt;li&gt;容量评估：基于全链路的压测手段、数据分布的模拟方法、关键场景调用量预估&lt;/li&gt;
&lt;li&gt;蓝绿发布：即多站点的灰度。具体操作流程：切流（将待发布机房流量切走）-&amp;gt; 机房发布（待发布机房全应用并行发布）-&amp;gt; 引流验证 （逐步按规则引流至100%）-&amp;gt;
  流量交换（将全部流程切换到已发布机房）-&amp;gt; 机房发布（另一个机房全应用并行发布）-&amp;gt; 分流还流（分流规则还原，两机房各50%）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;服务化&#34;&gt;服务化&lt;/h2&gt;

&lt;p&gt;今年IT界是对服务化异常的火爆，系统的稳定和流畅依赖好的应用架构，服务化治理如何规划和落地，是众多厂商系统的痛点。&lt;/p&gt;

&lt;p&gt;首先是来自1号店订单系统对SOA化的分享，SOA是一种架构模式,是设计原则,不是技术规范。狭义的SOA：Service化， 标准化、模块化、组件化。广义的SOA：模式、原则、思想。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Service化：1）分层结构，基础Service不含业务逻辑,只封装基本的数据操作。业务(聚合)Service封装业务逻辑甚至是全部的业务逻辑。2）Service层次调用，上层可以调用下层、下层不可调用上层、同层间可互相调用，调用链长度不超过3级、不循环调用。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;服务粒度划分：1）迷你裙定律。2）细粒度的服务(fine-grained)提供相对较小的功 能单元,或交换少量的数据。细粒度的服务使服务更容易被组装。3）粗粒度的服务(coarse-grained)则是在一个抽象 的接口中封装了独立的业务/技术能力,减少服务请求交互的次数。粗粒度的服务适合更广泛的需求。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;再次是来自Twitter的服务化思路分享：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;单体：牵一发而动全身&lt;/li&gt;
&lt;li&gt;分拆：把单体分成多个模块&lt;/li&gt;
&lt;li&gt;服务化：把模块按功能服务化&lt;/li&gt;
&lt;li&gt;平台化：模块功能中部分服务化为通用服务，通用服务提供一般化服务，平台化&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;docker&#34;&gt;Docker&lt;/h2&gt;

&lt;p&gt;在不断寻求性能更好、速度更快、成本更低的云计算核心技术中，容器技术是目前最吸引人注意的技术之一。尽管除去效率、速度和成本等方面的优势以外，容器技术还存在一些安全上需要斟酌的问题，但是其实际表现仍然得到了肯定。还是借用其中的分享内容来说明一下Docker。&lt;/p&gt;

&lt;p&gt;在遇到Docker之前：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;混乱的环境：Java, Golang, Ruby&lt;/li&gt;
&lt;li&gt;混乱的配置：Upstart, authorized_keys, dependency, 各种脚本&lt;/li&gt;
&lt;li&gt;混乱的监控：ErrorReporter, Message&lt;/li&gt;
&lt;li&gt;混乱的资源：计算资源与预估不匹配&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;导致的结果是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;环境不匹配导致,测试跟生产不一致&lt;/li&gt;
&lt;li&gt;配置混乱导致事故频发&lt;/li&gt;
&lt;li&gt;监控不统一导致运维难上加难&lt;/li&gt;
&lt;li&gt;资源效率低导致成本很高却达不到相应目标&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而Docker具有如下特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;构建快：应用+运行环境 = 镜像&lt;/li&gt;
&lt;li&gt;启动快：容器相比于虚机,更轻量级&lt;/li&gt;
&lt;li&gt;迁移快：应用以容器的方式标准化交付,标 准化运行&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;去年Docker主要是在吵作概念，而今年很多的互联网厂商已在使用Docker，本次Docker中都分享各自针对Docker的一些定制化修改及踩过的各种坑，所遇到的困难和走过的弯路。&lt;/p&gt;

&lt;p&gt;当然这些坑不是阻当我们不使用Docker的理由，Dockerk只是一个系统架构优化的承载体。来自Coding.net的分享最后总结的比较好，Docker会对软件，流程带入变革与影响，是否采用Docker，系统都需要关注如下三个方面，只是Docker让你不得不关注他们：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;软件架构的升级：微服务、无状态、数据执行分离&lt;/li&gt;
&lt;li&gt;研发体系、环境管理理念的升级：容器化、代码化、自动化&lt;/li&gt;
&lt;li&gt;资源管理理念的升级：Pet vs Cattle，多留点富余量，迁移能力比压榨能力更重要&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>参加CNUTCon全球容器大会感受</title>
      <link>http://lanlingzi.cn/post/technical/2015/0902_bj_cnutcon/</link>
      <pubDate>Wed, 02 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>http://lanlingzi.cn/post/technical/2015/0902_bj_cnutcon/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://lanlingzi.cn/images/docker/cnut.png&#34; alt=&#34;cnutcon&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于最近一直在从事Docker相关的工作，所以有机会参与这次的&lt;a href=&#34;http://cnutcon.com/&#34;&gt;CNUTCon全球容器大会&lt;/a&gt;。名字比较“高格”，虽有少量的外国人分享，大部分还是中国的互联网企业在宣传，忽悠。除去这些，整体来说这次大会还是非常不错的，门票也不算太贵，目前看来应该还是值的。我司还是这次大会的钻石赞助商，也说明我们在容器这一块的发力程度。&lt;/p&gt;

&lt;h2 id=&#34;整体感受&#34;&gt;整体感受&lt;/h2&gt;

&lt;p&gt;Docker是这这两年成长最快的技术，受到资本市场的热捧。Docker技术以势不可挡地席卷全球。参考这次大会，整体感受是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker已不再是概念，已进入互联网企业的实际生产环境中&lt;/li&gt;
&lt;li&gt;Docker的创业公司多，有远见的想在这次的浪潮中分享红利&lt;/li&gt;
&lt;li&gt;大公司借Docker东风，亦想在云计算领域中拿下更多话语权&lt;/li&gt;

&lt;li&gt;&lt;p&gt;容器技术处于战国群雄，完整的生态还比较混乱技术栈不成熟
&lt;/p&gt;

&lt;h2 id=&#34;看国外&#34;&gt;看国外&lt;/h2&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这次的CNUTCon，居然没有请求正牌的Docker公司，而是请到他的死对头CoreOS，其次还有RedHat，Google，以及Rancher。&lt;/p&gt;

&lt;p&gt;第一天的首场分享是来自RedHat副总裁，印度英语原来在公司就听到不少的印度同事，虽说听不太清楚，却有一股莫名的亲切感。由于是副总裁人物，讲的东西也是太High了，主要是分享OpenShift为什么要使用Docker，以及对Docker的认识。可以说在技术上空洞无物，对我来说“然并卵”。过程中的演示貌似险出了岔子。总之，他是来宣传OpenShift。&lt;/p&gt;

&lt;p&gt;其次是来自CoreOS产品负责人分享，不过也没有什么干货，可能他对国内Docker技术应用程度还不太了解，还停留在宣传概念阶段。主要讲了两组项目：一个是Chubby+Borg，之后是etcd+k8s。并分别对比了Chubby以及etcd，最后是基于etcd的使用演示，&lt;a href=&#34;https://github.com/kelseyhightower/cnutcon-2015&#34;&gt;Demo&lt;/a&gt;放到了Git上。只能说这个Demo是对etcd相当的入门级。总之，他是来宣传etcd。&lt;/p&gt;

&lt;p&gt;可以说，第一天的两场分享，其实跟Docker，或容器技术关联不是很大，看来InfoQ请错人了。&lt;/p&gt;

&lt;p&gt;第二天的来自国外的分享，有Google的华人美女工程师分享了“Kubernetes和Borg的设计哲学”。这一场还是不错的，虽也是比较High Level的介绍，不过让我这种屌丝有机会了解一下Google十多年前就开始的容器管理理论，感觉是真是简单实用：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;declarative &amp;gt; imperative&lt;/li&gt;
&lt;li&gt;Control loops&lt;/li&gt;
&lt;li&gt;Simple &amp;gt; Complex&lt;/li&gt;
&lt;li&gt;Modularity&lt;/li&gt;
&lt;li&gt;Legacy compatible&lt;/li&gt;
&lt;li&gt;Network-centric&lt;/li&gt;
&lt;li&gt;No grouping&lt;/li&gt;
&lt;li&gt;Cattle &amp;gt; Pets&lt;/li&gt;
&lt;li&gt;Open &amp;gt; Closed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;再次是来自Rancher Labs的秦总分享的“Rancher Labs 企业级私有容器服务平台解决方案分析”，并且他还跟我一起在现场的另外一个同事是之前的同事。干货比较多，演讲者虽说不懂技术，但Rancher给我带来是思维，尤其是后面介绍的“RancherOS”，在会场没有听太明白什么是“Dockerized OS”，后面查询一些资料，发现它除了内核之后，PID1就是Docker，其它的系统服务都Dockerized，并且在发行包的大小做到了极致，只有20M。能把Linux的系统服务通过Docker容器来管理，不得不说这这一项不错的创意，如果能实现应用在生产中，这不知又会对Linux产生什么样的深远影响。&lt;/p&gt;

&lt;h3 id=&#34;小结&#34;&gt;小结&lt;/h3&gt;

&lt;p&gt;由于只有四场，OpenShift与CoreOS是来做广告，我也曾经想在OpenShift免费空间上搭建Go的Web环境，发现真TMD的难用，OpenShift又想借Docker打个翻身仗，PaaS本身的体验不解决，Docker也“然并卵“。而CoreOS在容器中扮演着是一个搅局者，对防止Docker一家独大是益的，但它的RKT差不多落后Docker一年半，但是ETCD还是不错的。&lt;/p&gt;

&lt;p&gt;Google是老牌的容器使用者，他在这一这方面的经验可能是最具有发言权的。他也乘着Docker之势，迅速摧出K8S。并极力去构建Container Orchestration，ContainerInfrastructure，ContainerManagement的生态。虽说K8S目前还是很成熟，但在未来在容器界K8S必定举足轻重，甚于可能是Container Orchestration的事实领导者。&lt;/p&gt;

&lt;h2 id=&#34;看国内&#34;&gt;看国内&lt;/h2&gt;

&lt;p&gt;在国内，自然少不了BAT，以及后之秀京东。商业的成功驱动他们在技术上必定走在前列。第一天下午几场都使来自大厂的分享。&lt;/p&gt;

&lt;p&gt;首位是京东云平台的分享，京东最初的希望是通过一个平台，将物理机，虚拟机，容器，三种资源统一管理，随后的演化中，容器逐渐成为了一等公民。一种是容器直接在VM上，一种是让VM看起来像容器。开始是采用“胖容器”的模式，这一思路与我们的不谋而合，首先是要把容器使用起来，不管它是容器还是虚拟机；再次是业务的纯容器化。如何把容器中融合到已有系统中是目前大家遇到的最大挑战。&lt;/p&gt;

&lt;p&gt;其次是来自大众点评的分享，同样对容器的使用，也是使得容器看起来像虚拟机。重点介绍了在网络方面的经验，如通过新创建的br0网桥与eth连接，使得docker 容器可以有自己独立的IP。最后也分享在使用容器过程中一些坑。&lt;/p&gt;

&lt;p&gt;再次是阿里百川的TAE Docker全架构分享，干货是相当的多，信息量是相当的大的。Docker只是TAE中非常小的一部分，目前还是把Docker当做工具来用，重要介绍不是为了Docker而Docker，Docker并不等于容器。在实践的过程中，Docker的优势，基于Docker的全架构的PaaS平台，才兼具IaaS的灵活性和PaaS的易运维性。其实也说明Docker技术拉低了云平台的技术门槛，像原有的IaaS只有大投入才能玩得起，而Docker让你使用云资源变得更轻捷。&lt;/p&gt;

&lt;p&gt;再次是也来自腾讯的在游戏上，Docker实践：现状、经验及展望。其中有意义是在网络上的改造，目前Docker在网络上是很弱的。像点评一样，不得不面对网络打通的问题。一般来说，游戏业务的生命周期长短不一，这需要弹性的资源管理和交付。相比于虚拟机，容器更加轻量，效率更高，资源的交付和销毁更快。可能说像Docker的应用可对针对游戏业务提升资源的利用率，降低运营成本，也是Docker的魅力之一。&lt;/p&gt;

&lt;p&gt;第二天是来自百度的分享，感觉百度对于容器的实践比较牛逼，在docker没出来之前，他们就学Google都着眼于容器技术了。对于大企业来说，在资源调度上面对的困难是如何错峰填谷，如何将服务与机器解耦、预算调度，资源精细分配，统一池化，如何解决混合部署带来问题。而基于容器技术构建的Matrix平台，直接是在cgroup （划出一个资源框）namespace（内部的话只需要部分）的基础上定制操作。再通过agent来把这些所谓的“容器”启动起来，架构上有统一的container操作接口。其次是百度对于容器的安全性也有了很多实践，其实所说的安全性就是让容器上的代码不会跳到主机上去，让host上的代码不会逃逸出去。分享的内容很多，整体来说，百度应该是在国内互联网企业研究容器技术比较深的，而不仅仅是Docker的简单使用。&lt;/p&gt;

&lt;h3 id=&#34;小结-1&#34;&gt;小结&lt;/h3&gt;

&lt;p&gt;大公司的docker实践更有发言权，实际上他们对于docker的实践才是真正切合实际的，在实践过程中也是对于原有业务的相关性迁就比较多，不是为了容器而容器。各个公司解决方案，定制的过程，玩法，基本上是各有各的招。如遇到的网络的改进，渐进式的使用，某种程度上把docker当成虚拟机的来用。究其原因，还是因为业务解耦，平台自由，容器化的过程并没有那么简单。&lt;/p&gt;

&lt;h2 id=&#34;看编排调度&#34;&gt;看编排调度&lt;/h2&gt;

&lt;p&gt;这次会议上有几个都在分享K8S，Swarn的技术。由于一直关注K8S，我只是选择都听了一下。谈到K8S，大家都要说说mesos swarm，对比一番。后也听了我司的线超博对Swarn分享。整体来说，像K8s这种，还是理念较新的技术，大公司没有看到一个在采用，一是它出来太新了，二是在性能及稳定上存在问题。只有新创业的一些公司，赶上打着这些的旗号。一些散户玩玩还行，但对一企业级的业务，如何彻底地服务化，如何灵活地容器调度，原有业务如何契合，明显还有很长的路要走。而swarm更是不适合在生产环境中使用，并且docker公司想一家独食的原因，又要在编排上分一怀羹，目前大家都不看好它。另外swarm在设计上缺乏集群管理的视角，也难以在生产环境中发挥调度的优势。个人认为K8S在编排调度上会完胜Swarn。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>